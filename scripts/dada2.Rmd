---
title: "Dada2_report"
author: "Ram√≥n Gallego"
date: "1/9/2018"
output: html_document
params:
  folder: 
    value: /cloud/project/output.test/noprimers
  hash: TRUE
  trimming.length.Read1: 200
  trimming.length.Read2: 200
  metadata: output.metadata.csv
  output.folder: outputs

---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = params$folder)
```

## Dada2 report

You have successfully split your libraries into a pair (or two) of fastq files per sample. Now let's import the output of demultiplex_both_fastq.sh

First load the packages. And let's update the parameters we need
```{r loading packages, echo=TRUE ,message=FALSE}

library (tidyverse)
library (dada2)
library (digest)
library (seqinr)
library (knitr)
library (kableExtra)

sample.metadata <- read_csv(params$metadata)
filt_path <- file.path(params$folder, "filtered")
getwd()
filt_path
# Check if output directory exists - if not create as a subfolder of input dir

if(!dir.exists(file.path(params$output.folder))){
  dir.create(path = file.path(params$output.folder),recursive = T)
  output.dir <- file.path(params$output.folder)
}else{
  output.dir <- file.path(params$output.folder)
}
output.dir
```

```{r check quality trim point, message=FALSE, warning=FALSE}


sample.metadata %>%
  sample_n(4) %>% pull(file1) %>%
  plotQualityProfile(.)

sample.metadata %>%
  sample_n(4) %>% pull(file2) %>%
  plotQualityProfile(.)

```

The most common Amplicon length is 302, so trimming to 200 bp each read should give us enough overlap to successfully merge Forward and Reverse reads. Based on the drop of quality from the reverse read, you might choose to make them shorter on the R2. In that case, rerun the script setting the trimming differently.

```{r dadaing}

sample.metadata %>%
  mutate(filtF1 = file.path("filtered", str_replace(file1, "_L001_R1_001.fastq_sub.fastq", "_F1_filt.fastq.gz")),
         filtR1 = file.path("filtered", str_replace(file2, "_L001_R2_001.fastq_sub.fastq", "_R1_filt.fastq.gz"))) %>%
  mutate (outFs = pmap(.l= list (file1, filtF1, file2, filtR1),
                       .f = function(a, b, c, d) {
                         filterAndTrim(a,b,c,d,
                                       truncLen=c(params$trimming.length.Read1,params$trimming.length.Read2),
                                       maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                                       compress=TRUE, multithread=TRUE )
                       } ),
          errF1 = map(filtF1, ~ learnErrors(.x, multithread=TRUE,verbose = 0)),     # Calculate errors
          errR1 = map(filtR1, ~ learnErrors(.x, multithread=TRUE,verbose = 0)),
          derepF1 = map(filtF1, derepFastq),                   # dereplicate seqs
          derepR1 = map(filtR1, derepFastq),
          dadaF1  = map2(derepF1,errF1, ~ dada(.x, err = .y, multithread = TRUE)),  # dada2
          dadaR1  = map2(derepR1,errR1, ~ dada(.x, err = .y, multithread = TRUE)),
          mergers = pmap(.l = list(dadaF1,derepF1, dadaR1,derepR1),                 # merge things
                         .f = mergePairs )) -> output.dada2

write_rds(output.dada2, path = "output.halfway.rds")
seqtabF <- makeSequenceTable(output.dada2$mergers)
dim(seqtabF)

table(nchar(getSequences(seqtabF)))

```

Now we will remove chimera sequences. This is done within dada2 but you could use other ways

```{r RemovingChimeras, message=F}
seqtab.nochim <- removeBimeraDenovo(seqtabF, method="consensus", multithread=TRUE)
dim(seqtab.nochim)
seqtab.nochim.df <- as.data.frame(seqtab.nochim)
```


```{r setting up output files}
# Output files
  conv_file <- file.path(output.dir,"hash_key.csv")
  conv_file.fasta <- file.path(output.dir,"hash_key.fasta")

  ASV_file <-  file.path(output.dir,"ASV_table.csv")

  print (conv_file)
  print (conv_file.fasta)
  print(ASV_file)
```

```{r Hash or not}
if ( params$hash==TRUE)
  {
  conv_table <- tibble( Hash = "", Sequence ="")
   map_chr (colnames(seqtab.nochim.df), ~ digest(.x, algo = "sha1", serialize = F, skip = "auto")) -> Hashes
  conv_table <- tibble (Hash = Hashes,
                        Sequence = colnames(seqtab.nochim.df))
  
  colnames(seqtab.nochim.df) <- Hashes
  
  write_csv(conv_table, conv_file) # write the table into a file
  write.fasta(sequences = as.list(conv_table$Sequence),
              names     = as.list(conv_table$Hash),
              file.out = conv_file.fasta)
 seqtab.nochim.df <- bind_cols(sample.metadata %>%
                                select(Sample_name, Locus),
                              seqtab.nochim.df)
seqtab.nochim.df %>%
  pivot_longer(cols = c(- Sample_name, - Locus),
              names_to = "Hash",
              values_to = "nReads") %>%
  filter(nReads > 0) -> current_asv
write_csv(current_asv, ASV_file)    }else{
  #What do we do if you don't want hashes: two things - Change the header of the ASV table, write only one file
   seqtab.nochim.df %>%
  pivot_longer(cols = c(- Sample_name, - Locus),
              names_to = "Sequence",
              values_to = "nReads") %>%
  filter(nReads > 0) -> current_asv
write_csv(current_asv, ASV_file)
}
```


##Track the fate of all reads

```{r output_summary}

getN <- function(x) sum(getUniques(x))

output.dada2 %>%
  select(-file1, -file2, -filtF1, -filtR1, -errF1, -errR1, -derepF1, -derepR1) %>%
  mutate_at(.vars = c("dadaF1", "dadaR1", "mergers"),
            ~ sapply(.x,getN)) %>%
#  pull(outFs) -> test
  mutate(input = map_dbl(outFs, ~ .x[1]),
         filtered = map_dbl(outFs, ~ .x[2]),
         tabled  = rowSums(seqtabF),
         nonchim = rowSums(seqtab.nochim)) %>%
  select(Sample_name,
         Locus,
         input,
         filtered,
         denoised_F = dadaF1,
         denoised_R = dadaR1,
         merged = mergers,
         tabled,
         nonchim) -> track

write_csv(track, file.path(output.dir,"dada2_summary.csv"))

kable (track, align= "c", format = "html") %>%
      kable_styling(bootstrap_options= "striped", full_width = T, position = "center") %>%
      column_spec(2, bold=T)
track %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`, group =  Sample_name, color = Sample_name)) +
  geom_line() +
  facet_wrap(~Locus) +
  guides(color = "none")

```
